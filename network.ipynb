{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/home/saul/anaconda3/lib/python36.zip',\n",
       " '/home/saul/anaconda3/lib/python3.6',\n",
       " '/home/saul/anaconda3/lib/python3.6/lib-dynload',\n",
       " '/home/saul/anaconda3/lib/python3.6/site-packages',\n",
       " '/home/saul/anaconda3/lib/python3.6/site-packages/IPython/extensions',\n",
       " '/home/saul/.ipython',\n",
       " '/home/saul/PycharmProjects/Neural/venv/lib/python3.6/site-packages',\n",
       " '/home/saul/PycharmProjects/Neural/venv/lib/python3.6/site-packages']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.getcwd() + '/venv/lib/python3.6/site-packages')\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"сигмоидальная функция, работает и с числами, и с векторами (поэлементно)\"\"\"\n",
    "    return 1 / (1 + np.exp(round(-x, 3)))\n",
    "\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"производная сигмоидальной функции, работает и с числами, и с векторами (поэлементно)\"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "def threshold(x):\n",
    "    if x > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return max(x, 0)\n",
    "\n",
    "\n",
    "def softplus(x):\n",
    "    return math.log(1 + np.exp(round(x, 3)))\n",
    "\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def linear_derivative(x):\n",
    "    return 1\n",
    "\n",
    "\n",
    "def quadratic(x):\n",
    "    return x * x\n",
    "\n",
    "\n",
    "def quadratic_der(x):\n",
    "    return 2 * x\n",
    "\n",
    "\n",
    "def save(name='', fmt='png'):\n",
    "    pwd = os.getcwd()\n",
    "    path = '../resources/pictures/{}'.format(fmt)\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    os.chdir(path)\n",
    "    plt.savefig('{}.{}'.format(name, fmt), fmt=fmt)\n",
    "    os.chdir(pwd)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def print_graph(x, y):\n",
    "    fig = plt.figure()\n",
    "\n",
    "    plt.plot(x, y)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def read_matrix_from_file(file_name):\n",
    "    f = open(file_name)\n",
    "    matrix = []\n",
    "    while True:\n",
    "        line = f.readline()[:-1]\n",
    "        if len(line) == 0:\n",
    "            break\n",
    "        matrix_row = line.split(' ')\n",
    "        matrix.append([float(v) for v in matrix_row])\n",
    "    f.close()\n",
    "    return np.array(matrix)\n",
    "\n",
    "\n",
    "def init_random_weights_in_file(file_name, neuron_count, neuron_weights_count, range_from, range_to):\n",
    "    f = open(file_name, 'a')\n",
    "    for i in range(neuron_count):\n",
    "        res_str = ''\n",
    "        for j in range(neuron_weights_count):\n",
    "            res_str += str(round(random.uniform(range_from, range_to), 4)) + ' '\n",
    "        f.write(res_str[:-1] + '\\n')\n",
    "    f.write('\\n')\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def save_matrix_to_file(file_name, matrix, mode='w'):\n",
    "    f = open(file_name, mode)\n",
    "    for row in matrix:\n",
    "        f.write(np.array2string(row, max_line_width=10000, formatter={'float_kind': lambda x: \"%.6f\" % x})[1:-1] + '\\n')\n",
    "    f.write('\\n')\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def read_char_images_from_dir(directory_path, colorized, x, y):\n",
    "    res_arr = []\n",
    "    for filename in sorted(os.listdir(directory_path)):\n",
    "        f_path = directory_path + '/' + filename\n",
    "        if colorized:\n",
    "            res_arr.append([cv2.resize(cv2.imread(f_path, cv2.IMREAD_COLOR), (x, y))])\n",
    "        else:\n",
    "            res_arr.append([cv2.resize(cv2.imread(f_path, cv2.IMREAD_GRAYSCALE), (x, y))])\n",
    "    return np.array(res_arr)\n",
    "\n",
    "\n",
    "def read_all_char_examples_with_answers(directory_path, colorized, x=36, y=27):\n",
    "    dirs = os.listdir(directory_path)\n",
    "    diag = np.zeros((len(dirs), len(dirs)), int)\n",
    "    np.fill_diagonal(diag, 1)\n",
    "    examples = []\n",
    "    answers = []\n",
    "    for i, directory in enumerate(sorted(dirs)):\n",
    "        one_char_exs = read_char_images_from_dir\\\n",
    "            (directory_path + '/' + directory, colorized, x, y)\n",
    "        examples.extend(one_char_exs)\n",
    "        for _ in range(len(one_char_exs)):\n",
    "            answers.append(diag[i])\n",
    "    return np.array(examples), np.array(answers)\n",
    "\n",
    "\n",
    "def j_quadratic(y_hat, y):\n",
    "    \"\"\"\n",
    "    Оценивает значение квадратичной целевой функции.\n",
    "\n",
    "    y - матрица правильных ответов (n, N)\n",
    "    y_hat - матрица предсказаний (n, N)\n",
    "    Возвращает значение J (число)\n",
    "    \"\"\"\n",
    "\n",
    "    return 0.5 * np.mean((y_hat - y) ** 2)\n",
    "\n",
    "\n",
    "def j_quadratic_derivative(y_hat, y):\n",
    "    \"\"\"\n",
    "    Вычисляет матрицу частных производных целевой функции по каждому из предсказаний.\n",
    "    y - матрица правильных ответов (n, N)\n",
    "    y_hat - матрица предсказаний (n, N)\n",
    "    \"\"\"\n",
    "\n",
    "    return (y_hat - y) / (len(y) * y.shape[1])\n",
    "\n",
    "\n",
    "def j_cross_entropy(y_hat, y):\n",
    "    return -1 * np.mean(y * np.vectorize(math.log)(y_hat) + (1 - y) * np.vectorize(math.log)(1 - y_hat))\n",
    "\n",
    "\n",
    "def j_cross_entropy_derivative(y_hat, y):\n",
    "    return (y_hat - y) / (y_hat * (1 - y_hat) * len(y) * y.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "\n",
    "    def __init__(self, weights, activation_function=sigmoid, activation_function_derivative=sigmoid_prime):\n",
    "        \"\"\"\n",
    "        weights - вертикальный вектор весов нейрона формы (m, 1), weights[0][0] - смещение\n",
    "        activation_function - активационная функция нейрона, сигмоидальная функция по умолчанию\n",
    "        activation_function_derivative - производная активационной функции нейрона\n",
    "        \"\"\"\n",
    "\n",
    "        assert weights.shape[1] == 1, \"Incorrect weight shape\"\n",
    "\n",
    "        self.w = weights\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_function_derivative = activation_function_derivative\n",
    "\n",
    "    def forward_pass(self, single_input):\n",
    "        \"\"\"\n",
    "        активационная функция логистического нейрона\n",
    "        single_input - вектор входов формы (m, 1),\n",
    "        первый элемент вектора single_input - единица (если вы хотите учитывать смещение)\n",
    "        \"\"\"\n",
    "\n",
    "        result = 0\n",
    "        for i in range(self.w.size):\n",
    "            result += float(self.w[i] * single_input[i])\n",
    "        return self.activation_function(result)\n",
    "\n",
    "    def summatory(self, input_matrix):\n",
    "        \"\"\"\n",
    "        Вычисляет результат сумматорной функции для каждого примера из input_matrix.\n",
    "        input_matrix - матрица примеров размера (n, m) без смещения, каждая строка - отдельный пример,\n",
    "        n - количество примеров, m - количество переменных.\n",
    "        Возвращает вектор значений сумматорной функции размера (n, 1).\n",
    "        \"\"\"\n",
    "        return input_matrix.dot(self.w)\n",
    "\n",
    "    def activation(self, summatory_activation):\n",
    "        \"\"\"\n",
    "        Вычисляет для каждого примера результат активационной функции,\n",
    "        получив на вход вектор значений сумматорной функций\n",
    "        summatory_activation - вектор размера (n, 1),\n",
    "        где summatory_activation[i] - значение суммматорной функции для i-го примера.\n",
    "        Возвращает вектор размера (n, 1), содержащий в i-й строке\n",
    "        значение активационной функции для i-го примера.\n",
    "        \"\"\"\n",
    "        return np.vectorize(self.activation_function)(summatory_activation)\n",
    "\n",
    "    def derivative(self, summatory_activation):\n",
    "        \"\"\"\n",
    "        Вычисляет для каждого примера результат производной активационной функции,\n",
    "        получив на вход вектор значений сумматорной функций\n",
    "        summatory_activation - вектор размера (n, 1),\n",
    "        где summatory_activation[i] - значение суммматорной функции для i-го примера.\n",
    "        Возвращает вектор размера (n, 1), содержащий в i-й строке\n",
    "        значение производной активационной функции для i-го примера.\n",
    "        \"\"\"\n",
    "        return np.vectorize(self.activation_function_derivative)(summatory_activation)\n",
    "\n",
    "    def update_mini_batch(self, x, errors, learning_rate):\n",
    "        \"\"\"\n",
    "        x - матрица размера (batch_size, m)\n",
    "        errors - вектор ошибок (batch_size, 1)\n",
    "        learning_rate - константа скорости обучения\n",
    "        \"\"\"\n",
    "\n",
    "        grad = x.T.dot(errors)\n",
    "        self.w = self.w - grad * learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, neurons):\n",
    "        self.neurons = neurons\n",
    "        self.intermediate_sums = np.array([[]])\n",
    "        self.intermediate_activations = np.array([[]])\n",
    "\n",
    "    def process_input(self, input_matrix):\n",
    "        \"\"\"\n",
    "        :param input_matrix: Матрица входов для слоя, (n, m)\n",
    "        :return: Матрица активаций на выходе слоя\n",
    "        \"\"\"\n",
    "\n",
    "        sums = np.array([neuron.summatory(input_matrix).flatten()\n",
    "                         for neuron in self.neurons]).T\n",
    "        activations = np.array([v.activation(sums[:, i])\n",
    "                                for i, v in enumerate(self.neurons)]).T\n",
    "        self.intermediate_sums = sums\n",
    "        self.intermediate_activations = activations\n",
    "        return activations\n",
    "\n",
    "    def get_acts_by_sums_derivative(self):\n",
    "        \"\"\"\n",
    "        :return: Матрица производных активационных функций по суммам для каждого нейрона, (n, m)\n",
    "        \"\"\"\n",
    "        return np.array([v.derivative(self.intermediate_sums[:, i])\n",
    "                         for i, v in enumerate(self.neurons)]).T\n",
    "\n",
    "    def update_mini_batch(self, x, errors, learning_rate):\n",
    "        \"\"\"\n",
    "        x - матрица размера (batch_size, m)\n",
    "        errors - матрица ошибок (batch_size, N), N - число нейронов в слое\n",
    "        learning_rate - константа скорости обучения\n",
    "        \"\"\"\n",
    "\n",
    "        for i, v in enumerate(self.neurons):\n",
    "            v.update_mini_batch(x, errors[:, [i]], learning_rate)\n",
    "\n",
    "    def get_error(self, next_layer_errors, next_layer_weights):\n",
    "        \"\"\"\n",
    "        Считает ошибку на данном слое сети\n",
    "        next_layer_errors - ndarray размера (n, n_{l+1})\n",
    "        weights - ndarray размера (n_{l+1}, n_l+1)\n",
    "        :return: матрица ошибок (n, n_l)\n",
    "        \"\"\"\n",
    "\n",
    "        sums = self.intermediate_sums\n",
    "        sum_primes = np.array([v.derivative(sums[:, i])\n",
    "                               for i, v in enumerate(self.neurons)])\n",
    "        return (next_layer_weights[:, 1:].T.dot(next_layer_errors.T) * sum_primes).T\n",
    "\n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        :return: Матрица весов для всего слоя, (n_l, n_{l-1})\n",
    "        \"\"\"\n",
    "        return np.array([v.w.flatten() for i, v in enumerate(self.neurons)])\n",
    "\n",
    "    @staticmethod\n",
    "    def init_with_weights(weights, act_func, act_func_der):\n",
    "        return Layer(\n",
    "            [Neuron(weights_row.reshape(weights.shape[1], 1), act_func, act_func_der) for weights_row in weights])\n",
    "\n",
    "    @staticmethod\n",
    "    def layer_with_random_weights(neuron_count, weights_len, act_func, act_func_der):\n",
    "        return Layer([Neuron(np.array([[random.uniform(-1.0, 1.0)] for j in range(weights_len + 1)]),\n",
    "                             act_func, act_func_der) for i in range(neuron_count)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "\n",
    "    def __init__(self, layers, target_function, target_function_derivative):\n",
    "        self.layers = layers\n",
    "        self.target_function = target_function\n",
    "        self.target_function_derivative = target_function_derivative\n",
    "\n",
    "    def process_input(self, x, y):\n",
    "        \"\"\"\n",
    "        Выполняет один проход по сети для матрицы входных данных x.\n",
    "\n",
    "        :param x: Вектор входов, (n, m),\n",
    "        n - количество примеров\n",
    "        m - количество переменных(равно числу нейронов в 1-м слое сети)\n",
    "        :param y: Матрица правильных ответов, (n, N),\n",
    "        N - Число нейронов в выходном слое\n",
    "        :return: Значение целевой функции сети\n",
    "        \"\"\"\n",
    "\n",
    "        return self.target_function(self.get_result_matrix(x), y)\n",
    "\n",
    "    def get_result_matrix(self, x):\n",
    "        \"\"\"\n",
    "        Возвращает матрицу активаций выходного слоя\n",
    "        :param x: Матрица входов, (n, m)\n",
    "        :return: Матрица выходных активаций, (n, N)\n",
    "        \"\"\"\n",
    "\n",
    "        next_layer_input = np.append(np.ones((x.shape[0], 1), dtype=int), x, axis=1)\n",
    "        for layer in self.layers:\n",
    "            activations = layer.process_input(next_layer_input)\n",
    "            next_layer_input = np.append(np.ones((len(activations), 1), dtype=int), activations, axis=1)\n",
    "        return next_layer_input[:, 1:]\n",
    "\n",
    "    def sgd(self, x, y, batch_size, learning_rate, step_limit, eps=1e-6, visualize=False):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: Матрица входов - (n, m), n - число примеров, m - число переменных\n",
    "        :param y: Матрица правильных ответов - (n, N), n - число примеров,\n",
    "        N - число нейронов выходного слоя\n",
    "        :param batch_size: Размер батча, выбираемого для обучения\n",
    "        :param learning_rate: Константа скорости обучения\n",
    "        :param step_limit: Максимальное число шагов, которое может сделать алгоритм\n",
    "        :param eps: Точность алгоритма\n",
    "        :param visualize: Визуализировать ли изменение целевой функции\n",
    "        :return: 1, если успешно сошлось, 0 если достигнут предел количества допустимых шагов\n",
    "        \"\"\"\n",
    "\n",
    "        errors = 0\n",
    "        step = 0\n",
    "        steps = []\n",
    "        target_func_results = []\n",
    "        while (not errors) and (step < step_limit):\n",
    "            batch_ids_arr = Network.get_batches(np.arange(len(y)), batch_size)\n",
    "            init_target_func = self.process_input(x, y)\n",
    "            for batch_ids in batch_ids_arr:\n",
    "                x_b = x[batch_ids]\n",
    "                y_b = y[batch_ids]\n",
    "                self.update_mini_batch(x_b, y_b, learning_rate)\n",
    "                step += 1\n",
    "                if visualize:\n",
    "                    steps.append(step)\n",
    "                    target_func_results.append(self.process_input(x, y))\n",
    "            res_target_func = self.process_input(x, y)\n",
    "            errors += int(abs(init_target_func - res_target_func) < eps)\n",
    "        if visualize:\n",
    "            print_graph(steps, target_func_results)\n",
    "        return errors\n",
    "\n",
    "    def update_mini_batch(self, x, y, learning_rate):\n",
    "        \"\"\"\n",
    "        Обновляет все веса сети для одного батча\n",
    "        :param x: Матрица входов - (batch_size, m), batch_size - число примеров, m - число переменных\n",
    "        :param y: Матрица правильных ответов - (batch_size, N), batch_size - число примеров,\n",
    "        N - число нейронов выходного слоя\n",
    "        :param learning_rate: Константа скорости обучения\n",
    "        \"\"\"\n",
    "        result_matrix = self.get_result_matrix(x)\n",
    "        last_layer = self.layers[-1]\n",
    "        da_dz = last_layer.get_acts_by_sums_derivative()\n",
    "\n",
    "        layer_error = self.target_function_derivative(result_matrix, y) * da_dz\n",
    "        layer_weights = last_layer.get_weights()\n",
    "\n",
    "        last_layer_id = len(self.layers) - 1\n",
    "        reversed_layers = self.layers[::-1]\n",
    "        for i, v in enumerate(reversed_layers):\n",
    "            if i < last_layer_id:\n",
    "                layer_input = reversed_layers[i + 1].intermediate_activations\n",
    "            else:\n",
    "                layer_input = x\n",
    "            layer_input = np.append(np.ones((layer_input.shape[0], 1), dtype=int), layer_input, axis=1)\n",
    "\n",
    "            v.update_mini_batch(layer_input, layer_error, learning_rate)\n",
    "            if i < last_layer_id:\n",
    "                layer_error = reversed_layers[i + 1].get_error(layer_error, layer_weights)\n",
    "                layer_weights = reversed_layers[i + 1].get_weights()\n",
    "        return layer_error\n",
    "\n",
    "    @staticmethod\n",
    "    def get_batches(x, batch_size):\n",
    "        \"\"\"\n",
    "        Делит массив x на батчи размера n\n",
    "        :param x: Массив для деления\n",
    "        :param batch_size: Размер батча\n",
    "        :return: Массив батчей\n",
    "        \"\"\"\n",
    "        np.random.shuffle(x)\n",
    "        return np.array([x[i:i + batch_size] for i in range(0, len(x), batch_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.signal as signal\n",
    "\n",
    "\n",
    "class ConvolutionNeuron:\n",
    "    \"\"\"\n",
    "    Нейрон, сворачивающий карту с помощью ядра.\n",
    "    kernel - ядро для свертки.\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel, subsample_size=2):\n",
    "\n",
    "        assert kernel.shape[0] == kernel.shape[1] and kernel.shape[0] % 2 == 1, \"Kernel must be squared and odd-sized!\"\n",
    "\n",
    "        self.kernel = kernel\n",
    "        self.subsample_size = subsample_size\n",
    "        self.convolution_res = []\n",
    "\n",
    "    def process_sign_maps(self, sign_maps):\n",
    "        \"\"\"\n",
    "        Сворачивает входной массив карт признаков ядром и возвращает новый массив.\n",
    "        :param sign_maps: массив карт для свертки, одна карта разммера (n, m)\n",
    "        :return: массив новых карт, полученный в процессе свертки,\n",
    "         одна карта размера (n - kernel.x + 1, m - kernel.y + 1)\n",
    "        \"\"\"\n",
    "        self.convolution_res = []\n",
    "        return np.array([self.process_sign_map(sign_map) for sign_map in sign_maps])\n",
    "\n",
    "    def process_sign_map(self, sign_map):\n",
    "        \"\"\"\n",
    "        Сворачивает входную карту признаков ядром и возвращает новую карту.\n",
    "        :param sign_map: карта для свертки, (n, m)\n",
    "        :return: новая карта, полученный в процессе свертки, (n - kernel.x + 1, m - kernel.y + 1)\n",
    "        \"\"\"\n",
    "        res_map = signal.convolve2d(sign_map, self.kernel, 'valid')\n",
    "        self.convolution_res.append(res_map)\n",
    "        return self.subsample_one_map(res_map)\n",
    "\n",
    "    def update_mini_batch(self, neuron_input, error, learning_rate):\n",
    "        grad = np.zeros_like(self.kernel)\n",
    "        for i in range(neuron_input.shape[0]):\n",
    "            grad += self.get_grad_for_one_example(neuron_input[i], error[i])\n",
    "        self.kernel -= grad * learning_rate\n",
    "\n",
    "    def get_grad_for_one_example(self, neuron_input, error):\n",
    "        grad = np.zeros_like(self.kernel)\n",
    "        kernel_y = self.kernel.shape[0]\n",
    "        kernel_x = self.kernel.shape[1]\n",
    "        for y in range(error.shape[0]):\n",
    "            for x in range(error.shape[1]):\n",
    "                grad += neuron_input[y:y + kernel_y, x:x + kernel_x] * error[y, x]\n",
    "        return grad / (error.shape[0] * error.shape[1])\n",
    "\n",
    "    def subsample_one_map(self, sign_map):\n",
    "        ssize = self.subsample_size\n",
    "        output = np.zeros((int(math.ceil(sign_map.shape[0] / ssize)),\n",
    "                           int(math.ceil(sign_map.shape[1] / ssize))))\n",
    "        for y in range(output.shape[0]):\n",
    "            for x in range(output.shape[1]):\n",
    "                output[y, x] = np.amax(sign_map[y * ssize:y * ssize + ssize,\n",
    "                                       x * ssize:x * ssize + ssize])\n",
    "        return output\n",
    "\n",
    "    def create_err_map(self, next_layer_err, next_layer_weights, ex_id):\n",
    "        err_on_subs = self.create_err_map_for_subs(next_layer_err, next_layer_weights)\n",
    "        err_on_convs = self.create_err_map_for_conv(err_on_subs, ex_id)\n",
    "        return err_on_convs\n",
    "\n",
    "    @staticmethod\n",
    "    def create_err_map_for_subs(next_layer_err, next_layer_weights):\n",
    "        reversed_kernel = np.fliplr(np.flipud(next_layer_weights))\n",
    "        return signal.convolve2d(next_layer_err, reversed_kernel, 'full')\n",
    "\n",
    "    def create_err_map_for_conv(self, sub_layer_err, ex_id):\n",
    "        input_map = np.array(self.convolution_res)[ex_id]\n",
    "        max_values_primes_iter = iter(sub_layer_err.flatten())\n",
    "        output = np.zeros_like(input_map)\n",
    "        for y in range(sub_layer_err.shape[0]):\n",
    "            for x in range(sub_layer_err.shape[1]):\n",
    "                start_y = y * self.subsample_size\n",
    "                start_x = x * self.subsample_size\n",
    "                wind = input_map[start_y:start_y + self.subsample_size, start_x:start_x + self.subsample_size]\n",
    "                (a, b) = np.unravel_index(np.argmax(wind, axis=None), wind.shape)\n",
    "                oid_y = min(y * self.subsample_size + a, input_map.shape[0] - 1)\n",
    "                oid_x = min(x * self.subsample_size + b, input_map.shape[1] - 1)\n",
    "                output[oid_y, oid_x] = next(max_values_primes_iter)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionLayer:\n",
    "    \"\"\"\n",
    "    Класс, представляющий слой сверточной сети(сверки или подвыборки в зависимости от нейронов)\n",
    "    neurons = ядра свертки слоя\n",
    "    \"\"\"\n",
    "    def __init__(self, neurons):\n",
    "        self.neurons = neurons\n",
    "        self.saved_maps = []\n",
    "\n",
    "    def process_sign_maps(self, sign_maps_arr):\n",
    "        \"\"\"\n",
    "        Применяет каждый нейрон слоя к входному массиву карт признаков\n",
    "\n",
    "        :param sign_maps_arr: Входной массив карт признаков для каждого примера\n",
    "        :return: Массив с новыми картами признаков для каждого примера\n",
    "        \"\"\"\n",
    "        self.clear_convolution_results()\n",
    "        self.saved_maps = np.array([self.process_maps_one_example(maps) for maps in sign_maps_arr])\n",
    "        return self.saved_maps\n",
    "\n",
    "    def process_maps_one_example(self, maps):\n",
    "        res = []\n",
    "        neurons_for_map = int(len(self.neurons) / len(maps))\n",
    "        for i in range(len(maps)):\n",
    "            start = i * neurons_for_map\n",
    "            for neuron in self.neurons[start:start + neurons_for_map]:\n",
    "                res.append(neuron.process_sign_map(maps[i]))\n",
    "        return np.array(res)\n",
    "\n",
    "    def get_errors(self, next_layer_errs, next_layer_weights):\n",
    "        res = []\n",
    "        for ex_id in range(len(next_layer_errs)):\n",
    "            res.append(self.create_error_maps(next_layer_errs[ex_id], next_layer_weights, ex_id))\n",
    "        return np.array(res)\n",
    "\n",
    "    def create_error_maps(self, next_layer_errs, next_layer_weights, ex_id):\n",
    "        return np.array([neuron.create_err_map(next_layer_errs[i], next_layer_weights[i], ex_id)\n",
    "                        for i, neuron in enumerate(self.neurons)])\n",
    "\n",
    "    def update_mini_batch(self, layer_input, errors, learning_rate):\n",
    "        input_id = 0\n",
    "        neurons_for_map = int(len(self.neurons) / layer_input.shape[1])\n",
    "\n",
    "        for i, v in enumerate(self.neurons):\n",
    "            v.update_mini_batch(layer_input[:, input_id], errors[:, i], learning_rate)\n",
    "            if i == input_id * neurons_for_map + neurons_for_map:\n",
    "                input_id += 1\n",
    "\n",
    "    def get_weights(self):\n",
    "        return np.array([neuron.kernel for neuron in self.neurons])\n",
    "\n",
    "    def clear_convolution_results(self):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.convolution_res = []\n",
    "\n",
    "    @staticmethod\n",
    "    def init_random_convolution_layer(neuron_count, kernel_size, subsampling_size=2):\n",
    "        return ConvolutionLayer(np.array([ConvolutionNeuron(np.random.rand(kernel_size, kernel_size) - 0.5,\n",
    "                                                            subsampling_size) for i in range(neuron_count)]))\n",
    "\n",
    "    @staticmethod\n",
    "    def init_with_weights_from_file(file_name, kernel_sie, subsampling_size):\n",
    "        matrix = read_matrix_from_file(file_name)\n",
    "        return ConvolutionLayer(np.array([ConvolutionNeuron(v.reshape(kernel_sie, kernel_sie), subsampling_size)\n",
    "                                for i, v in enumerate(matrix)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionNetwork:\n",
    "    def __init__(self, layers, fully_connected_net):\n",
    "        self.layers = layers\n",
    "        self.fully_connected_net = fully_connected_net\n",
    "\n",
    "    def process_input(self, input_data, y):\n",
    "        next_layer_input = input_data\n",
    "        for layer in self.layers:\n",
    "            next_layer_input = layer.process_sign_maps(next_layer_input)\n",
    "\n",
    "        # next_layer_input = np.maximum(next_layer_input, 0)\n",
    "        return self.fully_connected_net.process_input(self.transform_maps(next_layer_input), y)\n",
    "\n",
    "    def sgd(self, x, y, batch_size, learning_rate, learning_rate_conn, step_limit, eps=1e-6, visualize=False):\n",
    "        errors = 0\n",
    "        step = 0\n",
    "        steps = []\n",
    "        target_func_results = []\n",
    "        while (not errors) and (step < step_limit):\n",
    "            batch_ids_arr = self.fully_connected_net.get_batches(np.arange(len(y)), batch_size)\n",
    "            init_target_func = self.process_input(x, y)\n",
    "            for batch_ids in batch_ids_arr:\n",
    "                x_b = x[batch_ids]\n",
    "                y_b = y[batch_ids]\n",
    "                self.update_mini_batch(x_b, y_b, learning_rate, learning_rate_conn)\n",
    "                step += 1\n",
    "                if visualize:\n",
    "                    steps.append(step)\n",
    "                    target_func_results.append(self.process_input(x, y))\n",
    "            res_target_func = self.process_input(x, y)\n",
    "            errors += int(abs(init_target_func - res_target_func) < eps)\n",
    "        if visualize:\n",
    "            print_graph(steps, target_func_results)\n",
    "        return errors\n",
    "\n",
    "    def update_mini_batch(self, x, y, learning_rate, learning_rate_conn):\n",
    "        self.process_input(x, y)\n",
    "        transformed = self.transform_maps(self.layers[-1].saved_maps)\n",
    "        errors = self.fully_connected_net.update_mini_batch(transformed, y, learning_rate_conn)\n",
    "        last_conv_layer_err = self.get_last_convolution_layer_err\\\n",
    "            (errors, self.fully_connected_net.layers[0].get_weights())\n",
    "\n",
    "        layer_err = last_conv_layer_err\n",
    "        last_layer_id = len(self.layers) - 1\n",
    "        reversed_layers = self.layers[::-1]\n",
    "        layer_weights = self.layers[-1].get_weights()\n",
    "\n",
    "        for i, v in enumerate(reversed_layers):\n",
    "            if i < last_layer_id:\n",
    "                layer_input = reversed_layers[i + 1].saved_maps\n",
    "            else:\n",
    "                layer_input = x\n",
    "            v.update_mini_batch(layer_input, layer_err, learning_rate)\n",
    "            if i < last_layer_id:\n",
    "                layer_err = reversed_layers[i + 1].get_errors(layer_err, layer_weights)\n",
    "                layer_weights = reversed_layers[i + 1].get_weights()\n",
    "\n",
    "        return layer_err\n",
    "\n",
    "    def get_last_convolution_layer_err(self, next_layer_errors, next_layer_weights):\n",
    "        last_layer = self.layers[-1]\n",
    "        last_layer_map_size = last_layer.saved_maps[0].shape[1:]\n",
    "\n",
    "        err_sub_lauer = (next_layer_weights[:, 1:].T.dot(next_layer_errors.T)).T\n",
    "        err_maps_sub_layer = ConvolutionNetwork.transform_err_arrays_to_maps\\\n",
    "            (err_sub_lauer, last_layer_map_size[0], last_layer_map_size[1])\n",
    "\n",
    "        final_result = []\n",
    "        for ex_id in range(len(next_layer_errors)):\n",
    "            ex_result = []\n",
    "            for i, neuron in enumerate(last_layer.neurons):\n",
    "                ex_result.append(neuron.create_err_map_for_conv(err_maps_sub_layer[ex_id][i], ex_id))\n",
    "            final_result.append(np.array(ex_result))\n",
    "        return np.array(final_result)\n",
    "\n",
    "    @staticmethod\n",
    "    def transform_maps(maps_arr):\n",
    "        return np.array([maps.flatten() for maps in maps_arr])\n",
    "\n",
    "    @staticmethod\n",
    "    def transform_err_arrays_to_maps(err_arrays, map_y, map_x):\n",
    "        return np.array([ConvolutionNetwork.transform_err_array_to_maps(err_arr, map_y, map_x)\n",
    "                         for err_arr in err_arrays])\n",
    "\n",
    "    @staticmethod\n",
    "    def transform_err_array_to_maps(err_arr, map_y, map_x):\n",
    "        elems_count = map_y * map_x\n",
    "        return np.array([err_arr[i:i + elems_count].reshape(map_y, map_x)\n",
    "                         for i in range(int(len(err_arr) / elems_count))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f542eb038b3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mrun_conv_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;31m# run_simple_net()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-f542eb038b3b>\u001b[0m in \u001b[0;36mrun_conv_net\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mexs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mexs\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m110\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfully_connected_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-a26924248864>\u001b[0m in \u001b[0;36msgd\u001b[0;34m(self, x, y, batch_size, learning_rate, learning_rate_conn, step_limit, eps, visualize)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mstep_limit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mbatch_ids_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfully_connected_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0minit_target_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_ids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_ids_arr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mx_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-a26924248864>\u001b[0m in \u001b[0;36mprocess_input\u001b[0;34m(self, input_data, y)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mnext_layer_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mnext_layer_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_sign_maps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_layer_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# next_layer_input = np.maximum(next_layer_input, 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-5a69205554ed>\u001b[0m in \u001b[0;36mprocess_sign_maps\u001b[0;34m(self, sign_maps_arr)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \"\"\"\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_convolution_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_maps_one_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmaps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msign_maps_arr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_maps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-5a69205554ed>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \"\"\"\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_convolution_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_maps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_maps_one_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmaps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msign_maps_arr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_maps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-5a69205554ed>\u001b[0m in \u001b[0;36mprocess_maps_one_example\u001b[0;34m(self, maps)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mneurons_for_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mneuron\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneurons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mneurons_for_map\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneuron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_sign_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-efc76cd825f8>\u001b[0m in \u001b[0;36mprocess_sign_map\u001b[0;34m(self, sign_map)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mres_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolve2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msign_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolution_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubsample_one_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_mini_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneuron_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-efc76cd825f8>\u001b[0m in \u001b[0;36msubsample_one_map\u001b[0;34m(self, sign_map)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 output[y, x] = np.amax(sign_map[y * ssize:y * ssize + ssize,\n\u001b[0;32m---> 59\u001b[0;31m                                        x * ssize:x * ssize + ssize])\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "def transform_input_imgs_to_data(imgs):\n",
    "    data = np.array(list((map(lambda arr: arr.flatten(), imgs)))) / 255\n",
    "    return abs(1 - data)\n",
    "\n",
    "\n",
    "def run_conv_net():\n",
    "    conv_layer1 = ConvolutionLayer.init_with_weights_from_file('resources/weights_kernels0', 5, 2)\n",
    "    conv_layer2 = ConvolutionLayer.init_with_weights_from_file('resources/weights_kernels1', 5, 2)\n",
    "    conv_layer3 = ConvolutionLayer.init_with_weights_from_file('resources/weights_kernels2', 3, 2)\n",
    "    # conv_layer1 = ConvolutionLayer.init_random_convolution_layer(8, 5, 2)\n",
    "    # conv_layer2 = ConvolutionLayer.init_random_convolution_layer(16, 5, 2)\n",
    "    # conv_layer3 = ConvolutionLayer.init_random_convolution_layer(32, 3, 2)\n",
    "    layer1 = Layer.layer_with_random_weights(100, 768, sigmoid, sigmoid_prime)\n",
    "    layer2 = Layer.layer_with_random_weights(10, 100, sigmoid, sigmoid_prime)\n",
    "\n",
    "    # network_conv = Network([layer1, layer2], j_cross_entropy, j_cross_entropy_derivative)\n",
    "    network_conv = Network(\n",
    "        [Layer.init_with_weights(read_matrix_from_file('resources/weights_conv' + str(i)), sigmoid, sigmoid_prime)\n",
    "         for i in range(2)], j_cross_entropy, j_cross_entropy_derivative)\n",
    "\n",
    "    network = ConvolutionNetwork([conv_layer1, conv_layer2, conv_layer3], network_conv)\n",
    "    exs, ans = read_all_char_examples_with_answers('resources/digits', False, 64, 48)\n",
    "    exs = abs(1 - exs / 255)\n",
    "\n",
    "    print(network.sgd(exs, ans, 110, 0.1, 0.1, 1, 1e-9, visualize=True))\n",
    "\n",
    "    for i, l in enumerate(network.fully_connected_net.layers):\n",
    "        save_matrix_to_file('resources/weights_conv' + str(i), l.get_weights())\n",
    "\n",
    "    for i, l in enumerate(network.layers):\n",
    "        save_matrix_to_file('resources/weights_kernels' + str(i), map(lambda x: x.flatten(), l.get_weights()))\n",
    "\n",
    "    print(network.process_input(exs, ans))\n",
    "\n",
    "\n",
    "def run_simple_net():\n",
    "    network = Network(\n",
    "        [Layer.init_with_weights(read_matrix_from_file('resources/weights' + str(i)), sigmoid, sigmoid_prime) for i in\n",
    "         range(2)],\n",
    "        j_cross_entropy,\n",
    "        j_cross_entropy_derivative)\n",
    "\n",
    "    # data = transform_input_imgs_to_data(read_char_images_from_dir('resources/test', False, 36, 27))\n",
    "    data, answers = read_all_char_examples_with_answers('resources/digits', False)\n",
    "\n",
    "    data = transform_input_imgs_to_data(data)\n",
    "    print(network.sgd(data, answers, 110, 10, 10, eps=1e-7, visualize=True))\n",
    "\n",
    "    for i, l in enumerate(network.layers):\n",
    "        save_matrix_to_file('resources/weights' + str(i), l.get_weights())\n",
    "\n",
    "    print(network.get_result_matrix(data))\n",
    "\n",
    "    print(network.process_input(data, answers))\n",
    "\n",
    "run_conv_net()\n",
    "# run_simple_net()\n",
    "sys.exit(0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
